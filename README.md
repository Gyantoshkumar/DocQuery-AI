# ğŸ¦™ DocQuery-AI (Local Version)

> A 100% local RAG system using Ollama + Llama 3.1 and Hugging Face Embeddings.

![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=ollama&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge)

## âš ï¸ Prerequisites

**You MUST install Ollama first:**
1. Download from [ollama.com/download](https://ollama.com/download)
2. Install the application
3. Open a **new terminal** and run:
   ```bash
   ollama pull llama3.1
   ```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run the App
```bash
streamlit run app.py
```
*If that doesn't work, try:*
```bash
python -m streamlit run app.py
```

### 3. Features
- **Upload PDFs**: Ingest multiple documents.
- **Chat**: Ask questions about your content.
- **Reset**: Use "Reset All" to clear documents and history.


## ğŸ› ï¸ Tech Stack

| Component | Technology | Local? |
|-----------|------------|--------|
| **LLM** | Ollama (Llama 3.1:8b) | âœ… Yes |
| **Embeddings** | HuggingFace (all-MiniLM-L6-v2) | âœ… Yes |
| **Vector DB** | FAISS | âœ… Yes |
| **UI** | Streamlit | âœ… Yes |

## ğŸ“ Structure

```
DocQuery-AI/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_engine.py    # Local RAG logic
â”‚   â””â”€â”€ config.py        # Model settings
â”œâ”€â”€ app.py               # UI
â””â”€â”€ requirements.txt     # Dependencies
```
